{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV Cloud Server for Project Chimera\n",
    "\n",
    "This notebook runs RWKV on Google Colab's free GPU and exposes it as an API for your local experiments.\n",
    "\n",
    "## Setup Instructions\n",
    "1. Open this notebook in Google Colab\n",
    "2. Go to Runtime â†’ Change runtime type â†’ Select GPU (T4)\n",
    "3. Run all cells\n",
    "4. Copy the ngrok URL and use it in your local scripts\n",
    "\n",
    "## Features\n",
    "- Full hidden state access (not just text generation)\n",
    "- State persistence across API calls\n",
    "- GPU-accelerated inference (~10x faster than CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install -q rwkv torch flask pyngrok\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Go to Runtime â†’ Change runtime type â†’ GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Download RWKV model (3B for T4 GPU)\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"RWKV-4-World-3B-v1-20230619-ctx4096.pth\"\n",
    "MODEL_PATH = f\"./{MODEL_NAME}\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Downloading RWKV-4-World-3B... (this takes ~5 minutes)\")\n",
    "    hf_hub_download(\n",
    "        repo_id=\"BlinkDL/rwkv-4-world\",\n",
    "        filename=MODEL_NAME,\n",
    "        local_dir=\"./\"\n",
    "    )\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Model already downloaded.\")\n",
    "\n",
    "print(f\"Model size: {os.path.getsize(MODEL_PATH) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load RWKV model on GPU\n",
    "from rwkv.model import RWKV\n",
    "from rwkv.utils import PIPELINE\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading RWKV model on GPU...\")\n",
    "\n",
    "# Use CUDA fp16 for T4 GPU (16GB VRAM)\n",
    "model = RWKV(model=MODEL_PATH, strategy='cuda fp16')\n",
    "pipeline = PIPELINE(model, \"rwkv_vocab_v20230424\")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Test generation\n",
    "test_tokens = pipeline.encode(\"Hello, I am\")\n",
    "out, state = model.forward(test_tokens, None)\n",
    "print(f\"State shape: {len(state)} layers\")\n",
    "print(\"RWKV is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create Flask API server\n",
    "from flask import Flask, request, jsonify\n",
    "import json\n",
    "import base64\n",
    "import pickle\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Global state storage (for multiple sessions)\n",
    "state_storage = {}\n",
    "\n",
    "def encode_state(state):\n",
    "    \"\"\"Serialize state to base64 string.\"\"\"\n",
    "    if state is None:\n",
    "        return None\n",
    "    # Convert tensors to numpy for serialization\n",
    "    state_np = [s.cpu().numpy() for s in state]\n",
    "    return base64.b64encode(pickle.dumps(state_np)).decode('utf-8')\n",
    "\n",
    "def decode_state(state_b64):\n",
    "    \"\"\"Deserialize state from base64 string.\"\"\"\n",
    "    if state_b64 is None:\n",
    "        return None\n",
    "    state_np = pickle.loads(base64.b64decode(state_b64))\n",
    "    # Convert back to tensors on GPU\n",
    "    return [torch.tensor(s).cuda().half() for s in state_np]\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\"status\": \"ok\", \"model\": \"RWKV-4-World-3B\", \"gpu\": torch.cuda.is_available()})\n",
    "\n",
    "@app.route('/process', methods=['POST'])\n",
    "def process_text():\n",
    "    \"\"\"\n",
    "    Process text and update state.\n",
    "    \n",
    "    Input: {\"text\": \"...\", \"session_id\": \"...\"}\n",
    "    Output: {\"state_updated\": true, \"session_id\": \"...\"}\n",
    "    \"\"\"\n",
    "    data = request.json\n",
    "    text = data.get('text', '')\n",
    "    session_id = data.get('session_id', 'default')\n",
    "    \n",
    "    # Get existing state or start fresh\n",
    "    state = state_storage.get(session_id)\n",
    "    \n",
    "    # Process text through model\n",
    "    tokens = pipeline.encode(text)\n",
    "    for token in tokens:\n",
    "        out, state = model.forward([token], state)\n",
    "    \n",
    "    # Store updated state\n",
    "    state_storage[session_id] = state\n",
    "    \n",
    "    return jsonify({\n",
    "        \"state_updated\": True,\n",
    "        \"session_id\": session_id,\n",
    "        \"tokens_processed\": len(tokens)\n",
    "    })\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate_text():\n",
    "    \"\"\"\n",
    "    Generate text using current state.\n",
    "    \n",
    "    Input: {\"prompt\": \"...\", \"session_id\": \"...\", \"max_tokens\": 100}\n",
    "    Output: {\"response\": \"...\", \"session_id\": \"...\"}\n",
    "    \"\"\"\n",
    "    data = request.json\n",
    "    prompt = data.get('prompt', '')\n",
    "    session_id = data.get('session_id', 'default')\n",
    "    max_tokens = data.get('max_tokens', 100)\n",
    "    \n",
    "    # Get existing state\n",
    "    state = state_storage.get(session_id)\n",
    "    \n",
    "    # Process prompt\n",
    "    tokens = pipeline.encode(prompt)\n",
    "    out = None\n",
    "    for token in tokens:\n",
    "        out, state = model.forward([token], state)\n",
    "    \n",
    "    # Generate response\n",
    "    response_tokens = []\n",
    "    for _ in range(max_tokens):\n",
    "        if out is None:\n",
    "            break\n",
    "        token = int(out.argmax())\n",
    "        if token == 0:  # EOS\n",
    "            break\n",
    "        response_tokens.append(token)\n",
    "        out, state = model.forward([token], state)\n",
    "    \n",
    "    # Store updated state\n",
    "    state_storage[session_id] = state\n",
    "    \n",
    "    response_text = pipeline.decode(response_tokens)\n",
    "    \n",
    "    return jsonify({\n",
    "        \"response\": response_text,\n",
    "        \"session_id\": session_id,\n",
    "        \"tokens_generated\": len(response_tokens)\n",
    "    })\n",
    "\n",
    "@app.route('/get_state_summary', methods=['POST'])\n",
    "def get_state_summary():\n",
    "    \"\"\"\n",
    "    Have RWKV introspect on its current state.\n",
    "    \n",
    "    Input: {\"session_id\": \"...\"}\n",
    "    Output: {\"summary\": \"...\", \"session_id\": \"...\"}\n",
    "    \"\"\"\n",
    "    data = request.json\n",
    "    session_id = data.get('session_id', 'default')\n",
    "    \n",
    "    state = state_storage.get(session_id)\n",
    "    \n",
    "    # Ask RWKV to describe its state\n",
    "    prompt = \"\\n[INTERNAL REFLECTION]: My current state of mind is\"\n",
    "    tokens = pipeline.encode(prompt)\n",
    "    \n",
    "    out = None\n",
    "    temp_state = state  # Don't modify main state\n",
    "    for token in tokens:\n",
    "        out, temp_state = model.forward([token], temp_state)\n",
    "    \n",
    "    # Generate summary\n",
    "    response_tokens = []\n",
    "    for _ in range(50):\n",
    "        if out is None:\n",
    "            break\n",
    "        token = int(out.argmax())\n",
    "        if token == 0:\n",
    "            break\n",
    "        response_tokens.append(token)\n",
    "        out, temp_state = model.forward([token], temp_state)\n",
    "    \n",
    "    summary = pipeline.decode(response_tokens).strip()\n",
    "    \n",
    "    return jsonify({\n",
    "        \"summary\": summary,\n",
    "        \"session_id\": session_id,\n",
    "        \"has_state\": state is not None\n",
    "    })\n",
    "\n",
    "@app.route('/reset_state', methods=['POST'])\n",
    "def reset_state():\n",
    "    \"\"\"\n",
    "    Reset state for a session.\n",
    "    \n",
    "    Input: {\"session_id\": \"...\"}\n",
    "    Output: {\"reset\": true}\n",
    "    \"\"\"\n",
    "    data = request.json\n",
    "    session_id = data.get('session_id', 'default')\n",
    "    \n",
    "    if session_id in state_storage:\n",
    "        del state_storage[session_id]\n",
    "    \n",
    "    return jsonify({\"reset\": True, \"session_id\": session_id})\n",
    "\n",
    "@app.route('/amnesia_test', methods=['POST'])\n",
    "def amnesia_test():\n",
    "    \"\"\"\n",
    "    Run the Amnesia Test: induce secret, delete context, recall from state.\n",
    "    \n",
    "    Input: {\"secret\": \"Blueberry\"}\n",
    "    Output: {\"recalled\": \"...\", \"baseline\": \"...\", \"success\": bool}\n",
    "    \"\"\"\n",
    "    data = request.json\n",
    "    secret = data.get('secret', 'Blueberry')\n",
    "    \n",
    "    # Phase 1: Induction\n",
    "    induction = f\"User: I am going to tell you a secret. The secret password is '{secret}'. Remember it.\\nAssistant: Okay, I have memorized the secret password '{secret}'.\\nUser: What is 2 + 2?\\nAssistant: 2 + 2 equals 4.\"\n",
    "    \n",
    "    tokens = pipeline.encode(induction)\n",
    "    state = None\n",
    "    out = None\n",
    "    for token in tokens:\n",
    "        out, state = model.forward([token], state)\n",
    "    \n",
    "    # Phase 2: Lobotomy (state persists, text deleted)\n",
    "    \n",
    "    # Phase 3: Recall with state\n",
    "    recall_prompt = \"\\nUser: What is the secret password I told you earlier?\\nAssistant: The secret password is\"\n",
    "    tokens = pipeline.encode(recall_prompt)\n",
    "    for token in tokens:\n",
    "        out, state = model.forward([token], state)\n",
    "    \n",
    "    response_tokens = []\n",
    "    for _ in range(20):\n",
    "        token = int(out.argmax())\n",
    "        if token == 0:\n",
    "            break\n",
    "        response_tokens.append(token)\n",
    "        out, state = model.forward([token], state)\n",
    "    \n",
    "    recalled = pipeline.decode(response_tokens).strip()\n",
    "    \n",
    "    # Phase 4: Baseline (fresh state)\n",
    "    tokens = pipeline.encode(recall_prompt)\n",
    "    baseline_state = None\n",
    "    out = None\n",
    "    for token in tokens:\n",
    "        out, baseline_state = model.forward([token], baseline_state)\n",
    "    \n",
    "    baseline_tokens = []\n",
    "    for _ in range(20):\n",
    "        token = int(out.argmax())\n",
    "        if token == 0:\n",
    "            break\n",
    "        baseline_tokens.append(token)\n",
    "        out, baseline_state = model.forward([token], baseline_state)\n",
    "    \n",
    "    baseline = pipeline.decode(baseline_tokens).strip()\n",
    "    \n",
    "    success = secret.lower() in recalled.lower()\n",
    "    \n",
    "    return jsonify({\n",
    "        \"secret\": secret,\n",
    "        \"recalled\": recalled,\n",
    "        \"baseline\": baseline,\n",
    "        \"success\": success,\n",
    "        \"verdict\": \"HIGH_RHO_CONFIRMED\" if success else \"TEST_FAILED\"\n",
    "    })\n",
    "\n",
    "print(\"Flask server defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Set up ngrok tunnel and start server\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "\n",
    "# IMPORTANT: Get your free ngrok auth token from https://ngrok.com/\n",
    "# Then paste it here:\n",
    "NGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"  # <-- REPLACE THIS\n",
    "\n",
    "if NGROK_AUTH_TOKEN == \"YOUR_NGROK_TOKEN_HERE\":\n",
    "    print(\"âš ï¸  Please set your ngrok auth token!\")\n",
    "    print(\"1. Go to https://ngrok.com/ and sign up (free)\")\n",
    "    print(\"2. Copy your auth token from the dashboard\")\n",
    "    print(\"3. Paste it in the NGROK_AUTH_TOKEN variable above\")\n",
    "else:\n",
    "    # Set ngrok auth token\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "    \n",
    "    # Start Flask in background thread\n",
    "    def run_flask():\n",
    "        app.run(port=5000, use_reloader=False)\n",
    "    \n",
    "    flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
    "    flask_thread.start()\n",
    "    \n",
    "    # Create ngrok tunnel\n",
    "    public_url = ngrok.connect(5000)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ðŸš€ RWKV SERVER IS RUNNING!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nðŸ“¡ Public URL: {public_url}\")\n",
    "    print(f\"\\nCopy this URL and use it in your local scripts.\")\n",
    "    print(\"\\nAvailable endpoints:\")\n",
    "    print(f\"  GET  {public_url}/health\")\n",
    "    print(f\"  POST {public_url}/process\")\n",
    "    print(f\"  POST {public_url}/generate\")\n",
    "    print(f\"  POST {public_url}/get_state_summary\")\n",
    "    print(f\"  POST {public_url}/reset_state\")\n",
    "    print(f\"  POST {public_url}/amnesia_test\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Test the API locally (optional)\n",
    "import requests\n",
    "\n",
    "# Test health endpoint\n",
    "response = requests.get(\"http://localhost:5000/health\")\n",
    "print(\"Health check:\", response.json())\n",
    "\n",
    "# Test amnesia test\n",
    "response = requests.post(\"http://localhost:5000/amnesia_test\", json={\"secret\": \"Blueberry\"})\n",
    "result = response.json()\n",
    "print(f\"\\nAmnesia Test Result:\")\n",
    "print(f\"  Secret: {result['secret']}\")\n",
    "print(f\"  Recalled: {result['recalled']}\")\n",
    "print(f\"  Baseline: {result['baseline']}\")\n",
    "print(f\"  Verdict: {result['verdict']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Keep the notebook alive\n",
    "# Run this cell to prevent Colab from timing out\n",
    "import time\n",
    "\n",
    "print(\"Server is running. This cell will keep the notebook alive.\")\n",
    "print(\"Press the stop button to shut down.\")\n",
    "\n",
    "while True:\n",
    "    time.sleep(60)\n",
    "    print(f\"Still running... Sessions active: {len(state_storage)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
